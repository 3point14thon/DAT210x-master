{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT210x - Programming with Python for DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module5- Lab5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.style.use('ggplot') # Look Pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Convenience Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(model, X, y):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    padding = 0.6\n",
    "    resolution = 0.0025\n",
    "    colors = ['royalblue','forestgreen','ghostwhite']\n",
    "\n",
    "    # Calculate the boundaris\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_min -= x_range * padding\n",
    "    y_min -= y_range * padding\n",
    "    x_max += x_range * padding\n",
    "    y_max += y_range * padding\n",
    "\n",
    "    # Create a 2D Grid Matrix. The values stored in the matrix\n",
    "    # are the predictions of the class at at said location\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                       np.arange(y_min, y_max, resolution))\n",
    "\n",
    "    # What class does the classifier say?\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour map\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.terrain)\n",
    "\n",
    "    # Plot the test original points as well...\n",
    "    for label in range(len(np.unique(y))):\n",
    "        indices = np.where(y == label)\n",
    "        plt.scatter(X[indices, 0], X[indices, 1], c=colors[label], label=str(label), alpha=0.8)\n",
    "\n",
    "    p = model.get_params()\n",
    "    plt.axis('tight')\n",
    "    plt.title('K = ' + str(p['n_neighbors']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load up the dataset into a variable called `X`. Check `.head` and `dtypes` to make sure you're loading your data properly--don't fail on the 1st step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id   area  perimeter  compactness  length  width  asymmetry  groove  \\\n",
      "0      0  15.26      14.84       0.8710   5.763  3.312     2.2210   5.220   \n",
      "1      1  14.88      14.57       0.8811   5.554  3.333     1.0180   4.956   \n",
      "2      2  14.29      14.09       0.9050   5.291  3.337     2.6990   4.825   \n",
      "3      3  13.84      13.94       0.8955   5.324  3.379     2.2590   4.805   \n",
      "4      4  16.14      14.99       0.9034   5.658  3.562     1.3550   5.175   \n",
      "5      5  14.38      14.21       0.8951   5.386  3.312     2.4620   4.956   \n",
      "6      6  14.69      14.49       0.8799   5.563  3.259     3.5860   5.219   \n",
      "7      7  14.11      14.10       0.8911   5.420  3.302     2.7000     NaN   \n",
      "8      8  16.63      15.46       0.8747   6.053  3.465     2.0400   5.877   \n",
      "9      9  16.44      15.25       0.8880   5.884  3.505     1.9690   5.533   \n",
      "10    10  15.26      14.85       0.8696   5.714  3.242     4.5430   5.314   \n",
      "11    11  14.03      14.16       0.8796   5.438  3.201     1.7170   5.001   \n",
      "12    12  13.89      14.02       0.8880   5.439  3.199     3.9860   4.738   \n",
      "13    13  13.78      14.06       0.8759   5.479  3.156     3.1360   4.872   \n",
      "14    14  13.74      14.05       0.8744   5.482  3.114     2.9320   4.825   \n",
      "15    15  14.59      14.28       0.8993   5.351  3.333     4.1850   4.781   \n",
      "16    16  13.99      13.83       0.9183   5.119  3.383     5.2340   4.781   \n",
      "17    17  15.69      14.75       0.9058   5.527  3.514     1.5990   5.046   \n",
      "18    18  14.70      14.21       0.9153   5.205  3.466     1.7670   4.649   \n",
      "19    19  12.72      13.57       0.8686   5.226  3.049     4.1020   4.914   \n",
      "20    20  14.16      14.40       0.8584   5.658  3.129     3.0720   5.176   \n",
      "21    21  14.11      14.26       0.8722   5.520  3.168     2.6880   5.219   \n",
      "22    22  15.88      14.90       0.8988   5.618  3.507     0.7651   5.091   \n",
      "23    23  12.08      13.23       0.8664   5.099  2.936     1.4150   4.961   \n",
      "24    24  15.01      14.76       0.8657   5.789  3.245     1.7910   5.001   \n",
      "25    25  16.19      15.16       0.8849   5.833  3.421     0.9030   5.307   \n",
      "26    26  13.02      13.76       0.8641   5.395  3.026     3.3730   4.825   \n",
      "27    27  12.74      13.67       0.8564   5.395  2.956     2.5040   4.869   \n",
      "28    28  14.11      14.18       0.8820   5.541  3.221     2.7540   5.038   \n",
      "29    29  13.45      14.02       0.8604   5.516  3.065     3.5310   5.097   \n",
      "..   ...    ...        ...          ...     ...    ...        ...     ...   \n",
      "180  180  11.41      12.95       0.8560   5.090  2.775     4.9570   4.825   \n",
      "181  181  12.46      13.41       0.8706   5.236  3.017     4.9870   5.147   \n",
      "182  182  12.19      13.36       0.8579   5.240  2.909     4.8570   5.158   \n",
      "183  183  11.65      13.07       0.8575   5.108  2.850     5.2090   5.135   \n",
      "184  184  12.89      13.77       0.8541   5.495  3.026     6.1850   5.316   \n",
      "185  185  11.56      13.31       0.8198   5.363  2.683     4.0620   5.182   \n",
      "186  186  11.81      13.45       0.8198   5.413  2.716     4.8980   5.352   \n",
      "187  187  10.91      12.80       0.8372   5.088  2.675     4.1790   4.956   \n",
      "188  188  11.23      12.82       0.8594   5.089  2.821     7.5240   4.957   \n",
      "189  189  10.59      12.41       0.8648   4.899  2.787     4.9750   4.794   \n",
      "190  190  10.93      12.80       0.8390   5.046  2.717     5.3980   5.045   \n",
      "191  191  11.27      12.86       0.8563   5.091  2.804     3.9850   5.001   \n",
      "192  192  11.87      13.02       0.8795   5.132  2.953     3.5970   5.132   \n",
      "193  193  10.82      12.83       0.8256   5.180  2.630     4.8530   5.089   \n",
      "194  194  12.11      13.27       0.8639   5.236  2.975     4.1320   5.012   \n",
      "195  195  12.80      13.47       0.8860   5.160  3.126     4.8730   4.914   \n",
      "196  196  12.79      13.53       0.8786   5.224  3.054     5.4830   4.958   \n",
      "197  197  13.37      13.78       0.8849   5.320  3.128     4.6700   5.091   \n",
      "198  198  12.62      13.67       0.8481   5.410  2.911     3.3060   5.231   \n",
      "199  199  12.76      13.38       0.8964   5.073  3.155     2.8280   4.830   \n",
      "200  200  12.38      13.44       0.8609   5.219  2.989     5.4720   5.045   \n",
      "201  201  12.67      13.32       0.8977   4.984  3.135     2.3000     NaN   \n",
      "202  202  11.18      12.72       0.8680   5.009  2.810     4.0510   4.828   \n",
      "203  203  12.70      13.41       0.8874   5.183  3.091     8.4560   5.000   \n",
      "204  204  12.37      13.47       0.8567   5.204  2.960     3.9190   5.001   \n",
      "205  205  12.19      13.20       0.8783   5.137  2.981     3.6310   4.870   \n",
      "206  206  11.23      12.88       0.8511   5.140  2.795     4.3250   5.003   \n",
      "207  207  13.20      13.66       0.8883   5.236  3.232     8.3150   5.056   \n",
      "208  208  11.84      13.21       0.8521   5.175  2.836     3.5980   5.044   \n",
      "209  209  12.30      13.34       0.8684   5.243  2.974     5.6370   5.063   \n",
      "\n",
      "    wheat_type  \n",
      "0         kama  \n",
      "1         kama  \n",
      "2         kama  \n",
      "3         kama  \n",
      "4         kama  \n",
      "5         kama  \n",
      "6         kama  \n",
      "7     canadian  \n",
      "8         kama  \n",
      "9         kama  \n",
      "10        kama  \n",
      "11        kama  \n",
      "12        kama  \n",
      "13        kama  \n",
      "14        kama  \n",
      "15        kama  \n",
      "16        kama  \n",
      "17        kama  \n",
      "18        kama  \n",
      "19        kama  \n",
      "20        kama  \n",
      "21        kama  \n",
      "22        kama  \n",
      "23        kama  \n",
      "24        kama  \n",
      "25        kama  \n",
      "26        kama  \n",
      "27        kama  \n",
      "28        kama  \n",
      "29        kama  \n",
      "..         ...  \n",
      "180   canadian  \n",
      "181   canadian  \n",
      "182   canadian  \n",
      "183   canadian  \n",
      "184   canadian  \n",
      "185   canadian  \n",
      "186   canadian  \n",
      "187   canadian  \n",
      "188   canadian  \n",
      "189   canadian  \n",
      "190   canadian  \n",
      "191   canadian  \n",
      "192   canadian  \n",
      "193   canadian  \n",
      "194   canadian  \n",
      "195   canadian  \n",
      "196   canadian  \n",
      "197   canadian  \n",
      "198   canadian  \n",
      "199   canadian  \n",
      "200   canadian  \n",
      "201   canadian  \n",
      "202   canadian  \n",
      "203   canadian  \n",
      "204   canadian  \n",
      "205   canadian  \n",
      "206   canadian  \n",
      "207   canadian  \n",
      "208   canadian  \n",
      "209   canadian  \n",
      "\n",
      "[210 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_csv('Datasets\\wheat.data')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the `wheat_type` series slice out of `X`, and into a series called `y`. Then drop the original `wheat_type` column from the `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x.wheat_type\n",
    "x = x.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick, \"ordinal\" conversion of `y`. In actuality our classification isn't ordinal, but just as an experiment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = y.astype('category').cat.codes\n",
    "y = y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some basic nan munging. Fill each row's nans with the mean of the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in x.columns:\n",
    "    x[column].fillna(x[column].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split `X` into training and testing data sets using `train_test_split()`. Use `0.33` test size, and use `random_state=1`. This is important so that your answers are verifiable. In the real world, you wouldn't specify a random_state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test, label_train, label_test =train_test_split(x,y,test_size=0.33,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of SKLearn's Normalizer class and then train it using its .fit() method against your _training_ data. The reason you only fit against your training data is because in a real-world situation, you'll only have your training data to train with! In this lab setting, you have both train+test data; but in the wild, you'll only have your training data, and then unlabeled data you want to apply your models to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = preprocessing.Normalizer().fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your trained pre-processor, transform both your training AND testing data. Any testing data has to be transformed with your preprocessor that has ben fit against your training data, so that it exist in the same feature-space as the original data used to train your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_trans = xt.transform(data_train)\n",
    "data_test_trans = xt.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Just like your preprocessing transformation, create a PCA transformation as well. Fit it against your training data, and then project your training and testing features into PCA space using the PCA model's `.transform()` method. This has to be done because the only way to visualize the decision boundary in 2D would be if your KNN algo ran in 2D as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "pca.fit(data_train_trans)\n",
    "dtrainpca = pca.transform(data_train_trans)\n",
    "dtestpca = pca.transform(data_test_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train a KNeighborsClassifier. Start with `K=9` neighbors. Be sure train your classifier against the pre-processed, PCA- transformed training data above! You do not, of course, need to transform your labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train = np.ravel(label_train)\n",
    "model = KNeighborsClassifier(n_neighbors=7)\n",
    "model.fit(dtrainpca,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEJCAYAAACdePCvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfc4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23f15700828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I hope your KNeighbors classifier model from earlier was named 'knn'\n",
    "# If not, adjust the following line:\n",
    "plotDecisionBoundary(model, dtrainpca, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the accuracy score of your test data/labels, computed by your KNeighbors model. You do NOT have to run `.predict` before calling `.score`, since `.score` will take care of running your predictions for you automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88571428571428568"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(dtestpca, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the ordinal conversion, try and get this assignment working with a proper Pandas get_dummies for feature encoding. You might have to update some of the `plotDecisionBoundary()` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "58px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
